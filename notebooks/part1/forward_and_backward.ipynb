{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will unveil the basic mechanism of the computational process of Analytics-Zoo using a simple linear regression example. In this example, we show how to obtain the gradients with a single forward and backward pass for updating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/lizhichao/bin/spark-2.1.0-bin-hadoop2.7\n",
      "Adding /home/lizhichao/bin/god/zoo/dist/lib/zoo-0.1.0-SNAPSHOT-jar-with-dependencies.jar to BIGDL_JARS\n",
      "Adding /home/lizhichao/bin/god/zoo/dist/lib/zoo-0.1.0-SNAPSHOT-jar-with-dependencies.jar to SPARK_CLASSPATH\n"
     ]
    }
   ],
   "source": [
    "from zoo.common.nncontext import *\n",
    "from bigdl.nn.criterion import *\n",
    "from zoo.pipeline.api.keras.layers import *\n",
    "from zoo.pipeline.api.keras.models import *\n",
    "from zoo.pipeline.api.autograd import *\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a simple linear regression which can be formulized as *y = Wx + b*ï¼Œ where *W = [w1,w2]* are weight parameters and *b* is the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createZooKerasSequential\n",
      "creating: createZooKerasDense\n",
      "{u'Lineard35147eb': {u'gradWeight': array([[ 0.,  0.]], dtype=float32), u'bias': array([ 0.], dtype=float32), u'weight': array([[ 0.59797329, -1.3809334 ]], dtype=float32), u'gradBias': array([ 0.], dtype=float32)}}\n",
      "(None, 2)\n",
      "(None, 1)\n"
     ]
    }
   ],
   "source": [
    "# the input data size is 2*1, the output size is 1*1\n",
    "linear = Sequential()\n",
    "dense = Dense(1, input_shape=[2])\n",
    "linear.add(dense)\n",
    "# print the randomly initialized parameters\n",
    "print(linear.parameters())\n",
    "print(linear.get_input_shape())\n",
    "print(linear.get_output_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.62202466]\n"
     ]
    }
   ],
   "source": [
    "input = np.array([1,-2])\n",
    "# forward to output\n",
    "output = linear.forward(input)\n",
    "print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we compute the error by using `AbsCrtierion` to measure the mean absolute value of the element-wise difference between input and target, then backpropagate the error of the predicted output to the input. If you want to try other criterions to compute the loss, click [here](https://bigdl-project.github.io/master/#APIGuide/Losses/) to see more details about our **Losses** API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createAbsCriterion\n",
      "loss: 1.6220247\n",
      "{u'Lineard35147eb': {u'gradWeight': array([[ 1., -2.]], dtype=float32), u'bias': array([ 0.], dtype=float32), u'weight': array([[ 0.59797329, -1.3809334 ]], dtype=float32), u'gradBias': array([ 1.], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "# mean absolute error\n",
    "mae = AbsCriterion()\n",
    "target = np.array([0])\n",
    "\n",
    "loss = mae.forward(output, target)\n",
    "print(\"loss: \" + str(loss))\n",
    "        \n",
    "grad_output = mae.backward(output, target)\n",
    "linear.backward(input, grad_output)\n",
    "\n",
    "print linear.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see that the backward pass has computed the gradient of the weights with respect to the loss. Therefore we can update the weights with the gradients using algorithms such as *stochastic gradient descent*. However in practice you **should** use *optimizer.optimize()* to circumvent the details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
