{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to introduce the *resilient distributed datasets* (RDDs) which is Spark's core abstraction when working with data. An RDD is a distributed collection of elements which can be operated on in parallel. Users can create RDD in two ways: *parallelizing* an existing collection in your driver program, or *loading* a dataset in an external storage system. RDDs support two types of operations: *transformations* and *actions*. Transformations construct a new RDD from a previous one and actions compute a result based on an RDD,. We introduce the basic operations of RDDs by the following simple word count example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/lizhichao/bin/spark-2.1.0-bin-hadoop2.7\n",
      "Adding /home/lizhichao/bin/god/zoo/dist/lib/zoo-0.1.0-SNAPSHOT-jar-with-dependencies.jar to BIGDL_JARS\n",
      "Adding /home/lizhichao/bin/god/zoo/dist/lib/zoo-0.1.0-SNAPSHOT-jar-with-dependencies.jar to SPARK_CLASSPATH\n"
     ]
    }
   ],
   "source": [
    "from zoo.common.nncontext import *\n",
    "sc = get_nncontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('world', 1)\n",
      "('hello', 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_file = sc.parallelize([\"hello\",\"hello world\"])\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "for line in counts.collect():\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line defines a base RDD by parallelizing an existing Python list. The second line defines *counts* as the result of a few transformations. In the third line and fourth line, the program print all elements from counts by calling *collect()*. *collect()* is used to retrieve the entire RDD if the data are expected to fit in memory. For more RDD APIs, you can refer to the website [RDD APIs](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
